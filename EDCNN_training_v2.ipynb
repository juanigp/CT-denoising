{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "REDCNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvuHsMtyEeO",
        "colab_type": "text"
      },
      "source": [
        "# Training of EDCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suh3h4QoyEeU",
        "colab_type": "code",
        "outputId": "73ee245e-4407-4a77-d72a-ddbc45576e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "!git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "%cd cloned-repo\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.EDCNN import EDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 250 (delta 121), reused 134 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (250/250), 40.47 MiB | 25.24 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "/content/cloned-repo\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4mkmrCNN-X",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters, model, dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504cgdHlyEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters:\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.00001\n",
        "\n",
        "#instantiating the model:\n",
        "model = EDCNN()\n",
        "\n",
        "#loss function\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "#optimizer algorithm\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "    \n",
        "#dataset and dataloaders\n",
        "#csv file containing the directories of the lo res and ground truth patches\n",
        "csv_file = r'/gdrive/My Drive/patches/100_FBPPhil.csv' \n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#split of data in training and testing data:\n",
        "#the .csv is shuffled (using the same seed everytime for repeatability)\n",
        "num_samples = len(dataset)\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "\n",
        "#pick 10% of samples to test\n",
        "testing_samples_percentage = 0.1\n",
        "split_index = int( num_samples * testing_samples_percentage )\n",
        "#pick the first 10% of samples in the shuffled dataset for testing\n",
        "testing_idx = total_idx[0 : split_index]\n",
        "#pick the other 90% of samples in the shuffled dataset for training\n",
        "training_idx = total_idx[split_index : num_samples]\n",
        "#random samplers for training and testing\n",
        "training_sampler = sampler.SubsetRandomSampler(training_idx)\n",
        "testing_sampler = sampler.SubsetRandomSampler(testing_idx)\n",
        "#dataloaders for training and testing\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = training_sampler)\n",
        "testing_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = testing_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdL61bkjOdoM",
        "colab_type": "text"
      },
      "source": [
        "## Training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKd4KwYOyEfN",
        "colab_type": "code",
        "outputId": "86188d5f-fe15-41b5-a38b-f84577e098b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#save a checkpoint of the model!\n",
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "#directory to save the models\n",
        "models_dir = r'/gdrive/My Drive/models'\n",
        "#file to record metrics  \n",
        "metrics_file_name = 'training_loss.csv' \n",
        "metrics_file_dir = os.path.join(models_dir, metrics_file_name)\n",
        "\n",
        "#loading a previously trained model\n",
        "resume_checkpoint = False\n",
        "#checkpoint_file_dir = \n",
        "if resume_checkpoint:\n",
        "  checkpoint = torch.load(checkpoint_file_dir)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "else:\n",
        "  start_epoch = 0\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    #training epoch\n",
        "    #training_epoch_loss = 0\n",
        "    #num_batches = 0\n",
        "    model.train()\n",
        "    for i, (lo_res, hi_res) in enumerate(training_dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "          continue  \n",
        "        num_batches += 1 \n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        #training_epoch_loss += loss.item()\n",
        "        #backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #training_epoch_loss /= num_batches\n",
        "    #print('Training epoch [{}/{}]'.format(epoch+1, num_epochs))\n",
        "\n",
        "    #save model after training epoch   \n",
        "    #checkpoint_file_dir = os.path.join(models_dir, 'EDCNN_checkpoint_epoch_' + str(epoch + 1) + '.pth.tar' )   \n",
        "    #save_checkpoint({\n",
        "    #    'epoch': epoch + 1,\n",
        "    #    'model': model.state_dict(),\n",
        "    #    'optimizer' : optimizer.state_dict(),\n",
        "    #}, checkpoint_file_dir)\n",
        "       \n",
        "#EVALUATION OF MODEL AFTER EPOCH\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      #ON TRAINING DATASET\n",
        "      training_epoch_loss = 0\n",
        "      num_batches = 0\n",
        "      for i, (lo_res, hi_res) in enumerate(training_dataloader):\n",
        "          #add an extra dimension:\n",
        "          lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "          hi_res = utils.var_or_cuda(hi_res)\n",
        "          if lo_res.size()[0] != batch_size:\n",
        "            continue  \n",
        "          num_batches += 1 \n",
        "          #forward pass \n",
        "          outputs = model(lo_res)\n",
        "          loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "          training_epoch_loss += loss.item()\n",
        "          #backward & optimize\n",
        "          #optimizer.zero_grad()\n",
        "          #loss.backward()\n",
        "          #optimizer.step()\n",
        "\n",
        "      training_epoch_loss /= num_batches\n",
        "      print('Training epoch [{}/{}]'.format(epoch+1, num_epochs))\n",
        "\n",
        "      #ON TESTING DATASET\n",
        "      testing_epoch_loss = 0\n",
        "      num_batches = 0\n",
        "      for batch, (lo_res, hi_res) in enumerate(testing_dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "            continue\n",
        "        num_batches += 1\n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        testing_epoch_loss += loss.item()\n",
        "\n",
        "    testing_epoch_loss /= num_batches\n",
        "    print('Testing epoch [{}/{}]'.format(epoch+1, num_epochs) )     \n",
        "\n",
        "    csv_line = str(training_epoch_loss) + ',' + str(testing_epoch_loss) + '\\n'\n",
        "    with open(metrics_file_dir , 'a+') as file:\n",
        "        file.write(csv_line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch [1/100]\n",
            "Testing epoch [1/100]\n",
            "Training epoch [2/100]\n",
            "Testing epoch [2/100]\n",
            "Training epoch [3/100]\n",
            "Testing epoch [3/100]\n",
            "Training epoch [4/100]\n",
            "Testing epoch [4/100]\n",
            "Training epoch [5/100]\n",
            "Testing epoch [5/100]\n",
            "Training epoch [6/100]\n",
            "Testing epoch [6/100]\n",
            "Training epoch [7/100]\n",
            "Testing epoch [7/100]\n",
            "Training epoch [8/100]\n",
            "Testing epoch [8/100]\n",
            "Training epoch [9/100]\n",
            "Testing epoch [9/100]\n",
            "Training epoch [10/100]\n",
            "Testing epoch [10/100]\n",
            "Training epoch [11/100]\n",
            "Testing epoch [11/100]\n",
            "Training epoch [12/100]\n",
            "Testing epoch [12/100]\n",
            "Training epoch [13/100]\n",
            "Testing epoch [13/100]\n",
            "Training epoch [14/100]\n",
            "Testing epoch [14/100]\n",
            "Training epoch [15/100]\n",
            "Testing epoch [15/100]\n",
            "Training epoch [16/100]\n",
            "Testing epoch [16/100]\n",
            "Training epoch [17/100]\n",
            "Testing epoch [17/100]\n",
            "Training epoch [18/100]\n",
            "Testing epoch [18/100]\n",
            "Training epoch [19/100]\n",
            "Testing epoch [19/100]\n",
            "Training epoch [20/100]\n",
            "Testing epoch [20/100]\n",
            "Training epoch [21/100]\n",
            "Testing epoch [21/100]\n",
            "Training epoch [22/100]\n",
            "Testing epoch [22/100]\n",
            "Training epoch [23/100]\n",
            "Testing epoch [23/100]\n",
            "Training epoch [24/100]\n",
            "Testing epoch [24/100]\n",
            "Training epoch [25/100]\n",
            "Testing epoch [25/100]\n",
            "Training epoch [26/100]\n",
            "Testing epoch [26/100]\n",
            "Training epoch [27/100]\n",
            "Testing epoch [27/100]\n",
            "Training epoch [28/100]\n",
            "Testing epoch [28/100]\n",
            "Training epoch [29/100]\n",
            "Testing epoch [29/100]\n",
            "Training epoch [30/100]\n",
            "Testing epoch [30/100]\n",
            "Training epoch [31/100]\n",
            "Testing epoch [31/100]\n",
            "Training epoch [32/100]\n",
            "Testing epoch [32/100]\n",
            "Training epoch [33/100]\n",
            "Testing epoch [33/100]\n",
            "Training epoch [34/100]\n",
            "Testing epoch [34/100]\n",
            "Training epoch [35/100]\n",
            "Testing epoch [35/100]\n",
            "Training epoch [36/100]\n",
            "Testing epoch [36/100]\n",
            "Training epoch [37/100]\n",
            "Testing epoch [37/100]\n",
            "Training epoch [38/100]\n",
            "Testing epoch [38/100]\n",
            "Training epoch [39/100]\n",
            "Testing epoch [39/100]\n",
            "Training epoch [40/100]\n",
            "Testing epoch [40/100]\n",
            "Training epoch [41/100]\n",
            "Testing epoch [41/100]\n",
            "Training epoch [42/100]\n",
            "Testing epoch [42/100]\n",
            "Training epoch [43/100]\n",
            "Testing epoch [43/100]\n",
            "Training epoch [44/100]\n",
            "Testing epoch [44/100]\n",
            "Training epoch [45/100]\n",
            "Testing epoch [45/100]\n",
            "Training epoch [46/100]\n",
            "Testing epoch [46/100]\n",
            "Training epoch [47/100]\n",
            "Testing epoch [47/100]\n",
            "Training epoch [48/100]\n",
            "Testing epoch [48/100]\n",
            "Training epoch [49/100]\n",
            "Testing epoch [49/100]\n",
            "Training epoch [50/100]\n",
            "Testing epoch [50/100]\n",
            "Training epoch [51/100]\n",
            "Testing epoch [51/100]\n",
            "Training epoch [52/100]\n",
            "Testing epoch [52/100]\n",
            "Training epoch [53/100]\n",
            "Testing epoch [53/100]\n",
            "Training epoch [54/100]\n",
            "Testing epoch [54/100]\n",
            "Training epoch [55/100]\n",
            "Testing epoch [55/100]\n",
            "Training epoch [56/100]\n",
            "Testing epoch [56/100]\n",
            "Training epoch [57/100]\n",
            "Testing epoch [57/100]\n",
            "Training epoch [58/100]\n",
            "Testing epoch [58/100]\n",
            "Training epoch [59/100]\n",
            "Testing epoch [59/100]\n",
            "Training epoch [60/100]\n",
            "Testing epoch [60/100]\n",
            "Training epoch [61/100]\n",
            "Testing epoch [61/100]\n",
            "Training epoch [62/100]\n",
            "Testing epoch [62/100]\n",
            "Training epoch [63/100]\n",
            "Testing epoch [63/100]\n",
            "Training epoch [64/100]\n",
            "Testing epoch [64/100]\n",
            "Training epoch [65/100]\n",
            "Testing epoch [65/100]\n",
            "Training epoch [66/100]\n",
            "Testing epoch [66/100]\n",
            "Training epoch [67/100]\n",
            "Testing epoch [67/100]\n",
            "Training epoch [68/100]\n",
            "Testing epoch [68/100]\n",
            "Training epoch [69/100]\n",
            "Testing epoch [69/100]\n",
            "Training epoch [70/100]\n",
            "Testing epoch [70/100]\n",
            "Training epoch [71/100]\n",
            "Testing epoch [71/100]\n",
            "Training epoch [72/100]\n",
            "Testing epoch [72/100]\n",
            "Training epoch [73/100]\n",
            "Testing epoch [73/100]\n",
            "Training epoch [74/100]\n",
            "Testing epoch [74/100]\n",
            "Training epoch [75/100]\n",
            "Testing epoch [75/100]\n",
            "Training epoch [76/100]\n",
            "Testing epoch [76/100]\n",
            "Training epoch [77/100]\n",
            "Testing epoch [77/100]\n",
            "Training epoch [78/100]\n",
            "Testing epoch [78/100]\n",
            "Training epoch [79/100]\n",
            "Testing epoch [79/100]\n",
            "Training epoch [80/100]\n",
            "Testing epoch [80/100]\n",
            "Training epoch [81/100]\n",
            "Testing epoch [81/100]\n",
            "Training epoch [82/100]\n",
            "Testing epoch [82/100]\n",
            "Training epoch [83/100]\n",
            "Testing epoch [83/100]\n",
            "Training epoch [84/100]\n",
            "Testing epoch [84/100]\n",
            "Training epoch [85/100]\n",
            "Testing epoch [85/100]\n",
            "Training epoch [86/100]\n",
            "Testing epoch [86/100]\n",
            "Training epoch [87/100]\n",
            "Testing epoch [87/100]\n",
            "Training epoch [88/100]\n",
            "Testing epoch [88/100]\n",
            "Training epoch [89/100]\n",
            "Testing epoch [89/100]\n",
            "Training epoch [90/100]\n",
            "Testing epoch [90/100]\n",
            "Training epoch [91/100]\n",
            "Testing epoch [91/100]\n",
            "Training epoch [92/100]\n",
            "Testing epoch [92/100]\n",
            "Training epoch [93/100]\n",
            "Testing epoch [93/100]\n",
            "Training epoch [94/100]\n",
            "Testing epoch [94/100]\n",
            "Training epoch [95/100]\n",
            "Testing epoch [95/100]\n",
            "Training epoch [96/100]\n",
            "Testing epoch [96/100]\n",
            "Training epoch [97/100]\n",
            "Testing epoch [97/100]\n",
            "Training epoch [98/100]\n",
            "Testing epoch [98/100]\n",
            "Training epoch [99/100]\n",
            "Testing epoch [99/100]\n",
            "Training epoch [100/100]\n",
            "Testing epoch [100/100]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}