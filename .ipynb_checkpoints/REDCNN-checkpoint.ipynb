{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- Work in progress\n",
    "- Separate data for training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "from models.Mini_REDCNN import REDCNN\n",
    "from utils import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from numpy import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wanna know how long does it take to train the net\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def time_diff(t_a, t_b):\n",
    "    t_diff = relativedelta(t_b, t_a) \n",
    "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the model:\n",
    "model = REDCNN()\n",
    "model.double()\n",
    "\n",
    "#loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#optimizer algorithm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#if gpu available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "#dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "csv_file = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images\\para probar el procesamiento de los volumenes\\100_FBPPhil_500FBP.csv'\n",
    "csv_file = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images\\patches\\100_FBPPhil_500FBP.csv'\n",
    "dataset = utils.CTVolumesDataset(csv_file)\n",
    "\n",
    "#split of data in training data and validation data\n",
    "num_samples = len(dataset)\n",
    "training_samples_percentage = 0.8\n",
    "split_index = int( num_samples * training_samples_percentage )\n",
    "total_idx = list(range(num_samples))\n",
    "random.seed(10)\n",
    "random.shuffle(total_idx)\n",
    "training_idx = total_idx[0 : split_index]\n",
    "validation_idx = total_idx[split_index : num_samples]\n",
    "training_sampler = sampler.SubsetRandomSampler(training_idx)\n",
    "\n",
    "#dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, sampler = training_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbatches = list(dataloader)\\n\\nprint(len(batches)) #how many batches\\nbatch = batches[0]\\nprint( len( batch ) ) #length of the batches (2 = lo res, hi res)\\nprint( batch[0].size() ) #size of the lo res volumes of the batch: batch_size volumes, size of volume\\nplt.imshow(batch[0][1][10][:][:], cmap = 'gray' )\\n\\n#enu = enumerate(dataloader)\\n#len(dataloader) # = amount of patches / batch size\\n\\n(lo_res, hi_res) = batch\\nprint(lo_res.size())\\nlo_res = lo_res.unsqueeze(1)\\nprint(lo_res.size())\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect training examples\n",
    "\"\"\"\n",
    "batches = list(dataloader)\n",
    "\n",
    "print(len(batches)) #how many batches\n",
    "batch = batches[0]\n",
    "print( len( batch ) ) #length of the batches (2 = lo res, hi res)\n",
    "print( batch[0].size() ) #size of the lo res volumes of the batch: batch_size volumes, size of volume\n",
    "plt.imshow(batch[0][1][10][:][:], cmap = 'gray' )\n",
    "\n",
    "#enu = enumerate(dataloader)\n",
    "#len(dataloader) # = amount of patches / batch size\n",
    "\n",
    "(lo_res, hi_res) = batch\n",
    "print(lo_res.size())\n",
    "lo_res = lo_res.unsqueeze(1)\n",
    "print(lo_res.size())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [1/794], Loss: 72613.4074\n",
      "Epoch [1/1000], Step [2/794], Loss: 69149.3773\n",
      "Epoch [1/1000], Step [3/794], Loss: 74489.3937\n",
      "Epoch [1/1000], Step [4/794], Loss: 69846.2536\n",
      "Epoch [1/1000], Step [5/794], Loss: 58626.8019\n",
      "Epoch [1/1000], Step [6/794], Loss: 48615.0876\n",
      "Epoch [1/1000], Step [7/794], Loss: 55968.9353\n",
      "Epoch [1/1000], Step [8/794], Loss: 60542.2035\n",
      "Epoch [1/1000], Step [9/794], Loss: 45586.7081\n",
      "Epoch [1/1000], Step [10/794], Loss: 47512.0354\n",
      "Epoch [1/1000], Step [11/794], Loss: 49353.4799\n",
      "Epoch [1/1000], Step [12/794], Loss: 40791.8499\n",
      "Epoch [1/1000], Step [13/794], Loss: 44873.5229\n",
      "Epoch [1/1000], Step [14/794], Loss: 40931.3018\n",
      "Epoch [1/1000], Step [15/794], Loss: 42087.9374\n",
      "Epoch [1/1000], Step [16/794], Loss: 45824.2772\n",
      "Epoch [1/1000], Step [17/794], Loss: 46910.0301\n",
      "Epoch [1/1000], Step [18/794], Loss: 58600.2251\n",
      "Epoch [1/1000], Step [19/794], Loss: 41012.1405\n",
      "Epoch [1/1000], Step [20/794], Loss: 37799.1358\n",
      "Epoch [1/1000], Step [21/794], Loss: 51781.2111\n",
      "Epoch [1/1000], Step [22/794], Loss: 41528.8546\n",
      "Epoch [1/1000], Step [23/794], Loss: 44658.3750\n",
      "Epoch [1/1000], Step [24/794], Loss: 34878.9970\n",
      "Epoch [1/1000], Step [25/794], Loss: 30134.9193\n",
      "Epoch [1/1000], Step [26/794], Loss: 33403.4436\n",
      "Epoch [1/1000], Step [27/794], Loss: 30299.0712\n",
      "Epoch [1/1000], Step [28/794], Loss: 30886.9592\n",
      "Epoch [1/1000], Step [29/794], Loss: 47347.3640\n",
      "Epoch [1/1000], Step [30/794], Loss: 54896.6189\n",
      "Epoch [1/1000], Step [31/794], Loss: 41925.1602\n",
      "Epoch [1/1000], Step [32/794], Loss: 41080.9116\n",
      "Epoch [1/1000], Step [33/794], Loss: 37016.9500\n",
      "Epoch [1/1000], Step [34/794], Loss: 45622.2506\n",
      "Epoch [1/1000], Step [35/794], Loss: 37466.3498\n",
      "Epoch [1/1000], Step [36/794], Loss: 35410.2663\n",
      "Epoch [1/1000], Step [37/794], Loss: 45848.3383\n",
      "Epoch [1/1000], Step [38/794], Loss: 35562.0490\n",
      "Epoch [1/1000], Step [39/794], Loss: 34090.7794\n",
      "Epoch [1/1000], Step [40/794], Loss: 34080.9301\n",
      "Epoch [1/1000], Step [41/794], Loss: 31786.6719\n",
      "Epoch [1/1000], Step [42/794], Loss: 35341.4795\n",
      "Epoch [1/1000], Step [43/794], Loss: 59308.6970\n",
      "Epoch [1/1000], Step [44/794], Loss: 36464.0755\n",
      "Epoch [1/1000], Step [45/794], Loss: 32238.0627\n",
      "Epoch [1/1000], Step [46/794], Loss: 33130.8337\n",
      "Epoch [1/1000], Step [47/794], Loss: 35119.1525\n",
      "Epoch [1/1000], Step [48/794], Loss: 36035.5105\n",
      "Epoch [1/1000], Step [49/794], Loss: 33882.1410\n",
      "Epoch [1/1000], Step [50/794], Loss: 34040.8144\n",
      "Epoch [1/1000], Step [51/794], Loss: 29789.2320\n",
      "Epoch [1/1000], Step [52/794], Loss: 47660.0612\n",
      "Epoch [1/1000], Step [53/794], Loss: 38228.3474\n",
      "Epoch [1/1000], Step [54/794], Loss: 33182.9753\n",
      "Epoch [1/1000], Step [55/794], Loss: 29938.3638\n",
      "Epoch [1/1000], Step [56/794], Loss: 33428.4584\n",
      "Epoch [1/1000], Step [57/794], Loss: 38330.3775\n",
      "Epoch [1/1000], Step [58/794], Loss: 31923.5030\n",
      "Epoch [1/1000], Step [59/794], Loss: 36344.4115\n",
      "Epoch [1/1000], Step [60/794], Loss: 29420.9952\n",
      "Epoch [1/1000], Step [61/794], Loss: 32067.8178\n",
      "Epoch [1/1000], Step [62/794], Loss: 33488.0696\n",
      "Epoch [1/1000], Step [63/794], Loss: 40170.4286\n",
      "Epoch [1/1000], Step [64/794], Loss: 26814.2437\n",
      "Epoch [1/1000], Step [65/794], Loss: 35691.1893\n",
      "Epoch [1/1000], Step [66/794], Loss: 35154.7401\n",
      "Epoch [1/1000], Step [67/794], Loss: 36351.7707\n",
      "Epoch [1/1000], Step [68/794], Loss: 34498.6087\n",
      "Epoch [1/1000], Step [69/794], Loss: 33443.7210\n",
      "Epoch [1/1000], Step [70/794], Loss: 38593.9930\n",
      "Epoch [1/1000], Step [71/794], Loss: 28732.8738\n",
      "Epoch [1/1000], Step [72/794], Loss: 34249.3557\n",
      "Epoch [1/1000], Step [73/794], Loss: 41558.3587\n",
      "Epoch [1/1000], Step [74/794], Loss: 34169.8550\n",
      "Epoch [1/1000], Step [75/794], Loss: 28445.7536\n"
     ]
    }
   ],
   "source": [
    "#directory to save the model\n",
    "models_dir = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images'\n",
    "file_name = 'metrics.csv' \n",
    "file_dir = os.path.join(models_dir,file_name)\n",
    "\n",
    "#set the model to train\n",
    "model.train()\n",
    "total_step = len(dataloader)\n",
    "\n",
    "#train\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    for i, (lo_res, hi_res) in enumerate(dataloader):\n",
    "        #add an extra dimension:\n",
    "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
    "        hi_res = utils.var_or_cuda(hi_res)\n",
    "        if lo_res.size()[0] != batch_size:\n",
    "            print(\"batch_size != {} drop last incompatible batch\".format( batch_size ))\n",
    "            continue\n",
    "            \n",
    "        #forward pass \n",
    "        outputs = model(lo_res)\n",
    "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
    "        #backward & optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #some people take the average of these values over the batch\n",
    "        running_loss += loss.item()\n",
    "        psnr = 10 * log10(1 / loss.item())\n",
    "        running_psnr += psnr\n",
    "        \n",
    "        if (i+1) % 1 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    #save model after epoch   \n",
    "    torch.save(model.state_dict(), os.path.join(models_dir, '3d_autoencoder.pkl_epoch_' + str(epoch) ) )      \n",
    "    \n",
    "    csv_line = str(running_loss) + ',' + str(running_psnr) + ',' + str(datetime.now()) + '\\n'\n",
    "    with open(file_dir , 'a+') as file:\n",
    "        file.write(csv_line)\n",
    "        \n",
    "        \n",
    "stop = datetime.now()\n",
    "dur = time_diff(start, stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
