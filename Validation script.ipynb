{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Validation script.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIth1W4lkKkY",
        "colab_type": "text"
      },
      "source": [
        "# Tuning of hyperparameters\n",
        "#### Script to train the model multiple times using different values for batch size and learning rate. The training is done using a small subset of the total training dataset and the results are validated against a different subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "45a85184-7640-4c3e-820b-4439054fde1c",
        "id": "G9if_8ekya5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "%cd cloned-repo\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.EDCNN import EDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cloned-repo' already exists and is not an empty directory.\n",
            "/content/cloned-repo\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npSXcEpVfCJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#csv file containing the directories of the lo res and ground truth patches\n",
        "csv_file = r'/gdrive/My Drive/patches/1.csv' \n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#split of data in training, validation and testing data:\n",
        "#the .csv is shuffled (using the same seed everytime for repeatability)\n",
        "num_samples = len(dataset)\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "\n",
        "#pick 10% of samples to test\n",
        "testing_samples_percentage = 0.1\n",
        "split_index = int( num_samples * testing_samples_percentage )\n",
        "#pick the first 10% of samples in the shuffled dataset for testing\n",
        "testing_idx = total_idx[0 : split_index]\n",
        "#pick the other 90% of samples in the shuffled dataset for training\n",
        "training_idx = total_idx[split_index : num_samples]\n",
        "#pick the first 10% of samples used for training. These are the samples that are going to be used for training in this script\n",
        "training_subset_idx = training_idx[0:split_index]\n",
        "#pick the second 10% of samples used for validation\n",
        "validation_subset_idx = training_idx[split_index: 2 * split_index]\n",
        "\n",
        "training_subset_sampler = sampler.SubsetRandomSampler(training_subset_idx)\n",
        "validation_sampler = sampler.SubsetRandomSampler(validation_subset_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6lgXcWfm54r",
        "colab_type": "code",
        "outputId": "a7b178c8-f434-4319-cbc9-a21c12c7538d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bs_list = [1, 2, 4, 8, 16, 32]\n",
        "lr_list = [0.01, 0.001, 0.0001, 0.00001]\n",
        "bs_np, lr_np = np.array(bs_list), np.array(lr_list)\n",
        "bs_mat, lr_mat = np.meshgrid(bs_np, lr_np)\n",
        "bs_np = np.resize(bs_mat, -1)\n",
        "lr_np = np.resize(lr_mat, -1)\n",
        "\n",
        "num_epochs = 30\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "#for i in range(len(lr_np)):\n",
        "for i in range(1):  \n",
        "  training_loss_list = []\n",
        "  validation_loss_list = []\n",
        "\n",
        "  lr = int(lr_np[i])\n",
        "  bs = int(bs_np[i])\n",
        "  lr = 0.00005\n",
        "  bs = 32\n",
        "\n",
        "  #dataloader\n",
        "  training_subset_dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size = bs, sampler = training_subset_sampler)\n",
        "  validation_dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size = bs, sampler = validation_sampler)\n",
        "  total_step = len(training_subset_dataloader)\n",
        "\n",
        "  model = EDCNN()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr)  \n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "      criterion.cuda()\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for batch, (lo_res, hi_res) in enumerate(training_subset_dataloader):\n",
        "      #add an extra dimension:\n",
        "      lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "      hi_res = utils.var_or_cuda(hi_res)\n",
        "      if lo_res.size()[0] != bs:\n",
        "          print(\"batch_size != {} drop last incompatible batch\".format( bs ))\n",
        "          continue\n",
        "\n",
        "      #forward pass \n",
        "      outputs = model(lo_res)\n",
        "      loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "      epoch_loss += loss\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (batch+1) % 1 == 0:\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, batch+1, total_step, loss.item()))    \n",
        "    training_loss_list.append(epoch_loss)\n",
        "    \n",
        "  \n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for batch, (lo_res, hi_res) in enumerate(validation_dataloader):\n",
        "\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != bs:\n",
        "            print(\"batch_size != {} drop last incompatible batch\".format( bs ))\n",
        "            continue\n",
        "\n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        epoch_loss += loss\n",
        "\n",
        "        if (batch+1) % 1 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, batch+1, total_step, loss.item()))\n",
        "      validation_loss_list.append(epoch_loss)\n",
        "    \n",
        "  training_losses.append(training_loss_list)\n",
        "  validation_losses.append(validation_loss_list)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [1/11], Loss: 145.9101\n",
            "Epoch [1/30], Step [2/11], Loss: 146.0580\n",
            "Epoch [1/30], Step [3/11], Loss: 153.1261\n",
            "Epoch [1/30], Step [4/11], Loss: 148.8514\n",
            "Epoch [1/30], Step [5/11], Loss: 147.6437\n",
            "Epoch [1/30], Step [6/11], Loss: 148.2355\n",
            "Epoch [1/30], Step [7/11], Loss: 147.2507\n",
            "Epoch [1/30], Step [8/11], Loss: 151.3611\n",
            "Epoch [1/30], Step [9/11], Loss: 154.0726\n",
            "Epoch [1/30], Step [10/11], Loss: 135.6117\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [1/30], Step [1/11], Loss: 144.5114\n",
            "Epoch [1/30], Step [2/11], Loss: 133.6452\n",
            "Epoch [1/30], Step [3/11], Loss: 137.4046\n",
            "Epoch [1/30], Step [4/11], Loss: 136.0155\n",
            "Epoch [1/30], Step [5/11], Loss: 139.2209\n",
            "Epoch [1/30], Step [6/11], Loss: 132.9982\n",
            "Epoch [1/30], Step [7/11], Loss: 143.0548\n",
            "Epoch [1/30], Step [8/11], Loss: 137.4522\n",
            "Epoch [1/30], Step [9/11], Loss: 127.6837\n",
            "Epoch [1/30], Step [10/11], Loss: 141.7758\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [2/30], Step [1/11], Loss: 136.0984\n",
            "Epoch [2/30], Step [2/11], Loss: 134.0316\n",
            "Epoch [2/30], Step [3/11], Loss: 133.3902\n",
            "Epoch [2/30], Step [4/11], Loss: 139.7010\n",
            "Epoch [2/30], Step [5/11], Loss: 146.3077\n",
            "Epoch [2/30], Step [6/11], Loss: 132.5663\n",
            "Epoch [2/30], Step [7/11], Loss: 141.1895\n",
            "Epoch [2/30], Step [8/11], Loss: 130.7682\n",
            "Epoch [2/30], Step [9/11], Loss: 135.5091\n",
            "Epoch [2/30], Step [10/11], Loss: 139.1651\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [2/30], Step [1/11], Loss: 129.1923\n",
            "Epoch [2/30], Step [2/11], Loss: 132.2115\n",
            "Epoch [2/30], Step [3/11], Loss: 132.4223\n",
            "Epoch [2/30], Step [4/11], Loss: 133.6876\n",
            "Epoch [2/30], Step [5/11], Loss: 139.4886\n",
            "Epoch [2/30], Step [6/11], Loss: 131.8502\n",
            "Epoch [2/30], Step [7/11], Loss: 131.6863\n",
            "Epoch [2/30], Step [8/11], Loss: 139.5808\n",
            "Epoch [2/30], Step [9/11], Loss: 137.9954\n",
            "Epoch [2/30], Step [10/11], Loss: 145.2125\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [3/30], Step [1/11], Loss: 138.5897\n",
            "Epoch [3/30], Step [2/11], Loss: 130.0860\n",
            "Epoch [3/30], Step [3/11], Loss: 135.1690\n",
            "Epoch [3/30], Step [4/11], Loss: 132.9738\n",
            "Epoch [3/30], Step [5/11], Loss: 133.8125\n",
            "Epoch [3/30], Step [6/11], Loss: 131.6057\n",
            "Epoch [3/30], Step [7/11], Loss: 134.8914\n",
            "Epoch [3/30], Step [8/11], Loss: 135.6049\n",
            "Epoch [3/30], Step [9/11], Loss: 128.8750\n",
            "Epoch [3/30], Step [10/11], Loss: 133.7549\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [3/30], Step [1/11], Loss: 132.1528\n",
            "Epoch [3/30], Step [2/11], Loss: 134.4189\n",
            "Epoch [3/30], Step [3/11], Loss: 131.3522\n",
            "Epoch [3/30], Step [4/11], Loss: 127.0702\n",
            "Epoch [3/30], Step [5/11], Loss: 112.2066\n",
            "Epoch [3/30], Step [6/11], Loss: 127.9121\n",
            "Epoch [3/30], Step [7/11], Loss: 140.6175\n",
            "Epoch [3/30], Step [8/11], Loss: 126.3568\n",
            "Epoch [3/30], Step [9/11], Loss: 133.3580\n",
            "Epoch [3/30], Step [10/11], Loss: 129.9512\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [4/30], Step [1/11], Loss: 129.8445\n",
            "Epoch [4/30], Step [2/11], Loss: 128.0041\n",
            "Epoch [4/30], Step [3/11], Loss: 123.4650\n",
            "Epoch [4/30], Step [4/11], Loss: 129.0069\n",
            "Epoch [4/30], Step [5/11], Loss: 126.9471\n",
            "Epoch [4/30], Step [6/11], Loss: 135.7594\n",
            "Epoch [4/30], Step [7/11], Loss: 130.4405\n",
            "Epoch [4/30], Step [8/11], Loss: 120.5896\n",
            "Epoch [4/30], Step [9/11], Loss: 123.1931\n",
            "Epoch [4/30], Step [10/11], Loss: 127.0397\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [4/30], Step [1/11], Loss: 123.8136\n",
            "Epoch [4/30], Step [2/11], Loss: 126.7384\n",
            "Epoch [4/30], Step [3/11], Loss: 115.3400\n",
            "Epoch [4/30], Step [4/11], Loss: 122.5845\n",
            "Epoch [4/30], Step [5/11], Loss: 124.9508\n",
            "Epoch [4/30], Step [6/11], Loss: 126.0526\n",
            "Epoch [4/30], Step [7/11], Loss: 122.7711\n",
            "Epoch [4/30], Step [8/11], Loss: 131.1134\n",
            "Epoch [4/30], Step [9/11], Loss: 124.0881\n",
            "Epoch [4/30], Step [10/11], Loss: 121.1615\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [5/30], Step [1/11], Loss: 130.5568\n",
            "Epoch [5/30], Step [2/11], Loss: 122.8341\n",
            "Epoch [5/30], Step [3/11], Loss: 117.5678\n",
            "Epoch [5/30], Step [4/11], Loss: 117.5646\n",
            "Epoch [5/30], Step [5/11], Loss: 125.8915\n",
            "Epoch [5/30], Step [6/11], Loss: 123.2542\n",
            "Epoch [5/30], Step [7/11], Loss: 128.1772\n",
            "Epoch [5/30], Step [8/11], Loss: 119.4582\n",
            "Epoch [5/30], Step [9/11], Loss: 123.9037\n",
            "Epoch [5/30], Step [10/11], Loss: 118.9636\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [5/30], Step [1/11], Loss: 123.5092\n",
            "Epoch [5/30], Step [2/11], Loss: 121.6656\n",
            "Epoch [5/30], Step [3/11], Loss: 114.1726\n",
            "Epoch [5/30], Step [4/11], Loss: 119.8936\n",
            "Epoch [5/30], Step [5/11], Loss: 120.5257\n",
            "Epoch [5/30], Step [6/11], Loss: 126.3550\n",
            "Epoch [5/30], Step [7/11], Loss: 116.6854\n",
            "Epoch [5/30], Step [8/11], Loss: 122.4289\n",
            "Epoch [5/30], Step [9/11], Loss: 121.1254\n",
            "Epoch [5/30], Step [10/11], Loss: 118.7190\n",
            "batch_size != 32 drop last incompatible batch\n",
            "Epoch [6/30], Step [1/11], Loss: 114.8452\n",
            "Epoch [6/30], Step [2/11], Loss: 120.2621\n",
            "Epoch [6/30], Step [3/11], Loss: 130.3110\n",
            "Epoch [6/30], Step [4/11], Loss: 119.6448\n",
            "Epoch [6/30], Step [5/11], Loss: 118.7610\n",
            "Epoch [6/30], Step [6/11], Loss: 110.1749\n",
            "Epoch [6/30], Step [7/11], Loss: 114.7721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9f0a2bf88416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n\u001b[0;32m---> 58\u001b[0;31m               .format(epoch+1, num_epochs, batch+1, total_step, loss.item()))    \n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtraining_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKAxvWMgSbkP",
        "colab_type": "code",
        "outputId": "0ea42ded-db28-40ce-fbf2-8f7ecbe313bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "len(training_loss_list)\n",
        "plt.plot(np.array(validation_loss_list)/(30*20), color = 'blue')\n",
        "plt.plot(np.array(training_loss_list)/(30*20), color = 'olive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f04ee3149e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/1TLMrwlcSl9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQZjYijRXdur",
        "colab_type": "code",
        "outputId": "b5247f7f-8af3-458c-b535-2928d6f5d83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "asd = []\n",
        "for i in range(10):\n",
        "  asd.append(0)\n",
        "  asd[-1] += i\n",
        "print (asd)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}