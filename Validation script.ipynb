{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Validation script.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIth1W4lkKkY",
        "colab_type": "text"
      },
      "source": [
        "# Tuning of hyperparameters\n",
        "#### Script to train the model multiple times using different values for batch size and learning rate. The training is done using a small subset of the total training dataset and the results are validated against a different subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "278e7f39-cc55-4375-dd17-94eb50ce4bce",
        "id": "G9if_8ekya5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "%cd cloned-repo\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.EDCNN import EDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cloned-repo' already exists and is not an empty directory.\n",
            "/content/cloned-repo\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npSXcEpVfCJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#csv file containing the directories of the lo res and ground truth patches\n",
        "csv_file = r'/gdrive/My Drive/patches/1.csv' \n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#split of data in training, validation and testing data:\n",
        "#the .csv is shuffled (using the same seed everytime for repeatability)\n",
        "num_samples = len(dataset)\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "\n",
        "#pick 10% of samples to test\n",
        "testing_samples_percentage = 0.1\n",
        "split_index = int( num_samples * testing_samples_percentage )\n",
        "#pick the first 10% of samples in the shuffled dataset for testing\n",
        "testing_idx = total_idx[0 : split_index]\n",
        "#pick the other 90% of samples in the shuffled dataset for training\n",
        "training_idx = total_idx[split_index : num_samples]\n",
        "#pick the first 10% of samples used for training. These are the samples that are going to be used for training in this script\n",
        "training_subset_idx = training_idx[0:split_index]\n",
        "#pick the second 10% of samples used for validation\n",
        "validation_subset_idx = training_idx[split_index: 2 * split_index]\n",
        "\n",
        "training_subset_sampler = sampler.SubsetRandomSampler(training_subset_idx)\n",
        "validation_sampler = sampler.SubsetRandomSampler(validation_subset_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6lgXcWfm54r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs_list = [1, 2, 4, 8, 16, 32]\n",
        "lr_list = [0.01, 0.001, 0.0001, 0.00001]\n",
        "bs_np, lr_np = np.array(bs_list), np.array(lr_list)\n",
        "bs_mat, lr_mat = np.meshgrid(bs_np, lr_np)\n",
        "bs_np = np.resize(bs_mat, -1)\n",
        "lr_np = np.resize(lr_mat, -1)\n",
        "\n",
        "num_epochs = 100\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for i in range(len(lr_np)):\n",
        "  training_loss_list = []\n",
        "  validation_loss_list = []\n",
        "\n",
        "  lr = int(lr_np[i])\n",
        "  bs = int(bs_np[i])\n",
        "\n",
        "  #dataloader\n",
        "  training_subset_dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size = bs, sampler = training_subset_sampler)\n",
        "  validation_dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size = bs, sampler = validation_sampler)\n",
        "  total_step = len(training_subset_dataloader)\n",
        "\n",
        "  model = EDCNN()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr)  \n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "      criterion.cuda()\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch, (lo_res, hi_res) in enumerate(training_subset_dataloader):\n",
        "      #add an extra dimension:\n",
        "      lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "      hi_res = utils.var_or_cuda(hi_res)\n",
        "      if lo_res.size()[0] != bs:\n",
        "          print(\"batch_size != {} drop last incompatible batch\".format( bs ))\n",
        "          continue\n",
        "\n",
        "      #forward pass \n",
        "      outputs = model(lo_res)\n",
        "      loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "      epoch_loss += loss\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (batch+1) % 1 == 0:\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, batch+1, total_step, loss.item()))    \n",
        "    training_loss_list.append(epoch_loss)\n",
        "\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    for batch, (lo_res, hi_res) in enumerate(validation_dataloader):\n",
        "\n",
        "      #add an extra dimension:\n",
        "      lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "      hi_res = utils.var_or_cuda(hi_res)\n",
        "      if lo_res.size()[0] != bs:\n",
        "          print(\"batch_size != {} drop last incompatible batch\".format( bs ))\n",
        "          continue\n",
        "\n",
        "      #forward pass \n",
        "      outputs = model(lo_res)\n",
        "      loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "      epoch_loss += loss\n",
        "\n",
        "      if (batch+1) % 1 == 0:\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, batch+1, total_step, loss.item()))\n",
        "    validation_loss_list.append(epoch_loss)\n",
        "    \n",
        "  training_losses.append(training_loss_list)\n",
        "  validation_losses.append(validation_loss_list)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}