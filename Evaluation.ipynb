{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM81RAJyQXgmxuIZ8k07RpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanigp/CT-denoising/blob/master/Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Av5PuaE_k2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "8d438351-6b1e-4e60-ded5-41221ec7b163"
      },
      "source": [
        "!git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "%cd cloned-repo\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.EDCNN import EDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 291 (delta 10), reused 0 (delta 0), pack-reused 270\u001b[K\n",
            "Receiving objects: 100% (291/291), 41.96 MiB | 23.12 MiB/s, done.\n",
            "Resolving deltas: 100% (145/145), done.\n",
            "/content/cloned-repo\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S86als-FYaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_dir = r'/gdrive/My Drive/models/'\n",
        "models_list = os.listdir(models_dir)\n",
        "models_100 = [os.path.join(models_dir,model) for model in models_list if 'EDCNN_checkpoint' in model]\n",
        "models_250 = [os.path.join(models_dir,model) for model in models_list if 'EDCNN_250_checkpoint' in model]\n",
        "models_360 = [os.path.join(models_dir,model) for model in models_list if 'EDCNN_360_checkpoint' in model]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdSXcc_8LNkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "194fd7c9-a528-45a0-dfe1-ce0df5ab3933"
      },
      "source": [
        "#llevarlo a funcion q tome lista de modelos\n",
        "\n",
        "#hyperparameters:\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.00001\n",
        "\n",
        "#instantiating the model:\n",
        "model = EDCNN()\n",
        "\n",
        "#loss function\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "#optimizer algorithm\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "\n",
        "csv_file = r'/gdrive/My Drive/patches/360_FBPPhil.csv' #######################\n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#split of data in training and testing data:\n",
        "#the .csv is shuffled (using the same seed everytime for repeatability)\n",
        "num_samples = len(dataset)\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "\n",
        "#pick 10% of samples to test\n",
        "testing_samples_percentage = 0.1\n",
        "split_index = int( num_samples * testing_samples_percentage )\n",
        "#pick the first 10% of samples in the shuffled dataset for testing\n",
        "testing_idx = total_idx[0 : split_index]\n",
        "#pick the other 90% of samples in the shuffled dataset for training\n",
        "training_idx = total_idx[split_index : num_samples]\n",
        "#random samplers for training and testing\n",
        "training_sampler = sampler.SubsetRandomSampler(training_idx)\n",
        "testing_sampler = sampler.SubsetRandomSampler(testing_idx)\n",
        "#dataloaders for training and testing\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = training_sampler)\n",
        "testing_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = testing_sampler)\n",
        "\n",
        "metrics_file_name = 'training_loss_360_.csv' #################################\n",
        "metrics_file_dir = os.path.join(models_dir, metrics_file_name)\n",
        "\n",
        "for checkpoint_dir in models_360:##############################################\n",
        "  checkpoint = torch.load(checkpoint_dir)\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    #ON TRAINING DATASET\n",
        "    training_epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    for i, (lo_res, hi_res) in enumerate(training_dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "          continue  \n",
        "        num_batches += 1 \n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        training_epoch_loss += loss.item()\n",
        "        #backward & optimize\n",
        "        #optimizer.zero_grad()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "\n",
        "    training_epoch_loss /= num_batches\n",
        "    print('Done training epoch')\n",
        "\n",
        "    #ON TESTING DATASET\n",
        "    testing_epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    for batch, (lo_res, hi_res) in enumerate(testing_dataloader):\n",
        "      #add an extra dimension:\n",
        "      lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "      hi_res = utils.var_or_cuda(hi_res)\n",
        "      if lo_res.size()[0] != batch_size:\n",
        "          continue\n",
        "      num_batches += 1\n",
        "      outputs = model(lo_res)\n",
        "      loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "      testing_epoch_loss += loss.item()\n",
        "\n",
        "    testing_epoch_loss /= num_batches\n",
        "    print('done testing epoch')     \n",
        "\n",
        "  csv_line = str(training_epoch_loss) + ',' + str(testing_epoch_loss) + '\\n'\n",
        "  with open(metrics_file_dir , 'a+') as file:\n",
        "      file.write(csv_line)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n",
            "Done training epoch\n",
            "done testing epoch\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}