{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "REDCNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvuHsMtyEeO",
        "colab_type": "text"
      },
      "source": [
        "# Training of REDCNN\n",
        "### Comments:\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suh3h4QoyEeU",
        "colab_type": "code",
        "outputId": "be86ee3f-d610-4d44-fad5-19c596b34b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "using_colab = True\n",
        "\n",
        "if using_colab :\n",
        "  !git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.Mini_REDCNN import REDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/175)\u001b[K\rremote: Counting objects:   1% (2/175)\u001b[K\rremote: Counting objects:   2% (4/175)\u001b[K\rremote: Counting objects:   3% (6/175)\u001b[K\rremote: Counting objects:   4% (7/175)\u001b[K\rremote: Counting objects:   5% (9/175)\u001b[K\rremote: Counting objects:   6% (11/175)\u001b[K\rremote: Counting objects:   7% (13/175)\u001b[K\rremote: Counting objects:   8% (14/175)\u001b[K\rremote: Counting objects:   9% (16/175)\u001b[K\rremote: Counting objects:  10% (18/175)\u001b[K\rremote: Counting objects:  11% (20/175)\u001b[K\rremote: Counting objects:  12% (21/175)\u001b[K\rremote: Counting objects:  13% (23/175)\u001b[K\rremote: Counting objects:  14% (25/175)\u001b[K\rremote: Counting objects:  15% (27/175)\u001b[K\rremote: Counting objects:  16% (28/175)\u001b[K\rremote: Counting objects:  17% (30/175)\u001b[K\rremote: Counting objects:  18% (32/175)\u001b[K\rremote: Counting objects:  19% (34/175)\u001b[K\rremote: Counting objects:  20% (35/175)\u001b[K\rremote: Counting objects:  21% (37/175)\u001b[K\rremote: Counting objects:  22% (39/175)\u001b[K\rremote: Counting objects:  23% (41/175)\u001b[K\rremote: Counting objects:  24% (42/175)\u001b[K\rremote: Counting objects:  25% (44/175)\u001b[K\rremote: Counting objects:  26% (46/175)\u001b[K\rremote: Counting objects:  27% (48/175)\u001b[K\rremote: Counting objects:  28% (49/175)\u001b[K\rremote: Counting objects:  29% (51/175)\u001b[K\rremote: Counting objects:  30% (53/175)\u001b[K\rremote: Counting objects:  31% (55/175)\u001b[K\rremote: Counting objects:  32% (56/175)\u001b[K\rremote: Counting objects:  33% (58/175)\u001b[K\rremote: Counting objects:  34% (60/175)\u001b[K\rremote: Counting objects:  35% (62/175)\u001b[K\rremote: Counting objects:  36% (63/175)\u001b[K\rremote: Counting objects:  37% (65/175)\u001b[K\rremote: Counting objects:  38% (67/175)\u001b[K\rremote: Counting objects:  39% (69/175)\u001b[K\rremote: Counting objects:  40% (70/175)\u001b[K\rremote: Counting objects:  41% (72/175)\u001b[K\rremote: Counting objects:  42% (74/175)\u001b[K\rremote: Counting objects:  43% (76/175)\u001b[K\rremote: Counting objects:  44% (77/175)\u001b[K\rremote: Counting objects:  45% (79/175)\u001b[K\rremote: Counting objects:  46% (81/175)\u001b[K\rremote: Counting objects:  47% (83/175)\u001b[K\rremote: Counting objects:  48% (84/175)\u001b[K\rremote: Counting objects:  49% (86/175)\u001b[K\rremote: Counting objects:  50% (88/175)\u001b[K\rremote: Counting objects:  51% (90/175)\u001b[K\rremote: Counting objects:  52% (91/175)\u001b[K\rremote: Counting objects:  53% (93/175)\u001b[K\rremote: Counting objects:  54% (95/175)\u001b[K\rremote: Counting objects:  55% (97/175)\u001b[K\rremote: Counting objects:  56% (98/175)\u001b[K\rremote: Counting objects:  57% (100/175)\u001b[K\rremote: Counting objects:  58% (102/175)\u001b[K\rremote: Counting objects:  59% (104/175)\u001b[K\rremote: Counting objects:  60% (105/175)\u001b[K\rremote: Counting objects:  61% (107/175)\u001b[K\rremote: Counting objects:  62% (109/175)\u001b[K\rremote: Counting objects:  63% (111/175)\u001b[K\rremote: Counting objects:  64% (112/175)\u001b[K\rremote: Counting objects:  65% (114/175)\u001b[K\rremote: Counting objects:  66% (116/175)\u001b[K\rremote: Counting objects:  67% (118/175)\u001b[K\rremote: Counting objects:  68% (119/175)\u001b[K\rremote: Counting objects:  69% (121/175)\u001b[K\rremote: Counting objects:  70% (123/175)\u001b[K\rremote: Counting objects:  71% (125/175)\u001b[K\rremote: Counting objects:  72% (126/175)\u001b[K\rremote: Counting objects:  73% (128/175)\u001b[K\rremote: Counting objects:  74% (130/175)\u001b[K\rremote: Counting objects:  75% (132/175)\u001b[K\rremote: Counting objects:  76% (133/175)\u001b[K\rremote: Counting objects:  77% (135/175)\u001b[K\rremote: Counting objects:  78% (137/175)\u001b[K\rremote: Counting objects:  79% (139/175)\u001b[K\rremote: Counting objects:  80% (140/175)\u001b[K\rremote: Counting objects:  81% (142/175)\u001b[K\rremote: Counting objects:  82% (144/175)\u001b[K\rremote: Counting objects:  83% (146/175)\u001b[K\rremote: Counting objects:  84% (147/175)\u001b[K\rremote: Counting objects:  85% (149/175)\u001b[K\rremote: Counting objects:  86% (151/175)\u001b[K\rremote: Counting objects:  87% (153/175)\u001b[K\rremote: Counting objects:  88% (154/175)\u001b[K\rremote: Counting objects:  89% (156/175)\u001b[K\rremote: Counting objects:  90% (158/175)\u001b[K\rremote: Counting objects:  91% (160/175)\u001b[K\rremote: Counting objects:  92% (161/175)\u001b[K\rremote: Counting objects:  93% (163/175)\u001b[K\rremote: Counting objects:  94% (165/175)\u001b[K\rremote: Counting objects:  95% (167/175)\u001b[K\rremote: Counting objects:  96% (168/175)\u001b[K\rremote: Counting objects:  97% (170/175)\u001b[K\rremote: Counting objects:  98% (172/175)\u001b[K\rremote: Counting objects:  99% (174/175)\u001b[K\rremote: Counting objects: 100% (175/175)\u001b[K\rremote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 175 (delta 82), reused 119 (delta 39), pack-reused 0\n",
            "Receiving objects: 100% (175/175), 39.06 MiB | 21.71 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "/content/cloned-repo/cloned-repo/cloned-repo/cloned-repo\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4mkmrCNN-X",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters, model, dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504cgdHlyEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters:\n",
        "num_epochs = 1000\n",
        "batch_size = 16\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#instantiating the model:\n",
        "model = REDCNN()\n",
        "#model.double()\n",
        "\n",
        "#loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#optimizer algorithm\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "    \n",
        "#dataset\n",
        "if using_colab:\n",
        "  csv_file = r'/gdrive/My Drive/patches/1.csv'  \n",
        "else:\n",
        "  #should be XCT instead of 500FBP!!\n",
        "  csv_file = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images\\patches\\100_FBPPhil_500FBP.csv'\n",
        "\n",
        "#dataset, dataloader  \n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRHBRCjZOSpP",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRynJNjAyEfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "#inspect training examples\n",
        "\n",
        "batches = list(dataloader)\n",
        "\n",
        "print(len(batches)) #how many batches\n",
        "batch = batches[20]\n",
        "print( len( batch ) ) #length of the batches (2 = lo res, hi res)\n",
        "print( batch[0].size() ) #size of the lo res volumes of the batch: batch_size volumes, size of volume\n",
        "plt.imshow(batch[1][0][10][:][:], cmap = 'gray' )\n",
        "\n",
        "#enu = enumerate(dataloader)\n",
        "#len(dataloader) # = amount of patches / batch size\n",
        "\n",
        "(lo_res, hi_res) = batch\n",
        "print(lo_res.size())\n",
        "lo_res = lo_res.unsqueeze(1)\n",
        "print(lo_res.size())\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdL61bkjOdoM",
        "colab_type": "text"
      },
      "source": [
        "## Training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGdq4B34I0xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKd4KwYOyEfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#directory to save the model\n",
        "if using_colab:\n",
        "  models_dir = r'/gdrive/My Drive/models' \n",
        "else:\n",
        "  models_dir = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images'  \n",
        "\n",
        "#file to record metrics  \n",
        "metrics_file_name = 'metrics.csv' \n",
        "metrics_file_dir = os.path.join(models_dir, metrics_file_name)\n",
        "\n",
        "#loading a previously trained model\n",
        "resume_checkpoint = False\n",
        "checkpoint_file_dir = os.path.join(models_dir,'REDCNN_checkpoint_epoch_0.pth.tar')\n",
        "if resume_checkpoint:\n",
        "  checkpoint = torch.load(checkpoint_file_dir)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "else:\n",
        "  start_epoch = 0\n",
        "\n",
        "\n",
        "#training\n",
        "total_step = len(dataloader)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "  \n",
        "    for i, (lo_res, hi_res) in enumerate(dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "            print(\"batch_size != {} drop last incompatible batch\".format( batch_size ))\n",
        "            continue\n",
        "            \n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        #backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        if (i+1) % 1 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    #save model after epoch   \n",
        "    checkpoint_file_dir = os.path.join(models_dir, 'REDCNN_checkpoint_epoch_' + str(epoch + 1) + '.pth.tar' )\n",
        "    \n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, checkpoint_file_dir)\n",
        "\n",
        "    csv_line = str(loss.item()) + ',' + str(epoch) + '\\n'\n",
        "    with open(metrics_file_dir , 'a+') as file:\n",
        "        file.write(csv_line)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}