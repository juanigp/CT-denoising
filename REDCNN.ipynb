{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "REDCNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvuHsMtyEeO",
        "colab_type": "text"
      },
      "source": [
        "### Comments:\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suh3h4QoyEeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using_colab = True\n",
        "\n",
        "if using_colab :\n",
        "  !git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.Mini_REDCNN import REDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITHDXCc9yEei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wanna know how long does it take to train the net\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "def time_diff(t_a, t_b):\n",
        "    t_diff = relativedelta(t_b, t_a) \n",
        "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUiaGNwqyEer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters:\n",
        "num_epochs = 1000\n",
        "batch_size = 16\n",
        "learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504cgdHlyEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#instantiating the model:\n",
        "model = REDCNN()\n",
        "model.double()\n",
        "\n",
        "#loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#optimizer algorithm\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "#dataset, dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOUm8XOQyEe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset\n",
        "if using_colab:\n",
        "  csv_file = r'/gdrive/My Drive/patches/1.csv'  \n",
        "else:\n",
        "  csv_file = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images\\patches\\100_FBPPhil_500FBP.csv'\n",
        "\n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#this is no longer necessary\n",
        "\"\"\"\n",
        "#split of data in training data and validation data\n",
        "num_samples = len(dataset)\n",
        "training_samples_percentage = 0.8\n",
        "split_index = int( num_samples * training_samples_percentage )\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "training_idx = total_idx[0 : split_index]\n",
        "validation_idx = total_idx[split_index : num_samples]\n",
        "training_sampler = sampler.SubsetRandomSampler(training_idx)\n",
        "\"\"\"\n",
        "\n",
        "#dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, suffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRynJNjAyEfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "#inspect training examples\n",
        "\n",
        "batches = list(dataloader)\n",
        "\n",
        "print(len(batches)) #how many batches\n",
        "batch = batches[0]\n",
        "print( len( batch ) ) #length of the batches (2 = lo res, hi res)\n",
        "print( batch[0].size() ) #size of the lo res volumes of the batch: batch_size volumes, size of volume\n",
        "plt.imshow(batch[0][1][10][:][:], cmap = 'gray' )\n",
        "\n",
        "#enu = enumerate(dataloader)\n",
        "#len(dataloader) # = amount of patches / batch size\n",
        "\n",
        "(lo_res, hi_res) = batch\n",
        "print(lo_res.size())\n",
        "lo_res = lo_res.unsqueeze(1)\n",
        "print(lo_res.size())\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKd4KwYOyEfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#directory to save the model\n",
        "if using_colab:\n",
        "  models_dir = r'/gdrive/My Drive/models' \n",
        "else:\n",
        "  models_dir = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images'  \n",
        "\n",
        "file_name = 'metrics.csv' \n",
        "file_dir = os.path.join(models_dir,file_name)\n",
        "\n",
        "#set the model to train\n",
        "model.train()\n",
        "total_step = len(dataloader)\n",
        "\n",
        "#train\n",
        "start = datetime.now()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #running_mse = 0.0\n",
        "    #running_rmse = 0.0\n",
        "    #running_psnr = 0.0\n",
        "    for i, (lo_res, hi_res) in enumerate(dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "            print(\"batch_size != {} drop last incompatible batch\".format( batch_size ))\n",
        "            continue\n",
        "            \n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        #backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #record stats\n",
        "        #note: record to .csv\n",
        "        #running_mse += loss.item()\n",
        "        #running_rmse += np.sqrt(loss.item())\n",
        "        #psnr = 10 * np.log10(1 / loss.item()) #ver esto\n",
        "        #running_psnr += psnr\n",
        "        \n",
        "        if (i+1) % 1 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    #save model after epoch   \n",
        "    torch.save(model.state_dict(), os.path.join(models_dir, '3d_autoencoder_epoch_' + str(epoch) + '.pkl' ) )      \n",
        "    \n",
        "    #csv_line = str(running_mse) + ',' + str(running_psnr) + ',' + str(datetime.now()) + '\\n'\n",
        "    csv_line = str(loss.item()) + ',' + str(epoch) + '\\n'\n",
        "    with open(file_dir , 'a+') as file:\n",
        "        file.write(csv_line)\n",
        "        \n",
        "        \n",
        "stop = datetime.now()\n",
        "dur = time_diff(start, stop)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}