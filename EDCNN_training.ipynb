{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "REDCNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvuHsMtyEeO",
        "colab_type": "text"
      },
      "source": [
        "# Training of EDCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suh3h4QoyEeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone -l -s git://github.com/juanigp/CT-denoising.git cloned-repo\n",
        "%cd cloned-repo\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)\n",
        "\n",
        "import os\n",
        "from IPython.core.debugger import set_trace\n",
        "from models.EDCNN import EDCNN\n",
        "from utils import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4mkmrCNN-X",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters, model, dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504cgdHlyEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters:\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.00001\n",
        "\n",
        "#instantiating the model:\n",
        "model = EDCNN()\n",
        "\n",
        "#loss function\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "#optimizer algorithm\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "    \n",
        "#dataset and dataloaders\n",
        "#csv file containing the directories of the lo res and ground truth patches\n",
        "csv_file = r'/gdrive/My Drive/patches/100_FBPPhil.csv' \n",
        "dataset = utils.CTVolumesDataset(csv_file)\n",
        "\n",
        "#split of data in training and testing data:\n",
        "#the .csv is shuffled (using the same seed everytime for repeatability)\n",
        "num_samples = len(dataset)\n",
        "total_idx = list(range(num_samples))\n",
        "random.seed(10)\n",
        "random.shuffle(total_idx)\n",
        "\n",
        "#pick 10% of samples to test\n",
        "testing_samples_percentage = 0.1\n",
        "split_index = int( num_samples * testing_samples_percentage )\n",
        "#pick the first 10% of samples in the shuffled dataset for testing\n",
        "testing_idx = total_idx[0 : split_index]\n",
        "#pick the other 90% of samples in the shuffled dataset for training\n",
        "training_idx = total_idx[split_index : num_samples]\n",
        "#random samplers for training and testing\n",
        "training_sampler = sampler.SubsetRandomSampler(training_idx)\n",
        "testing_sampler = sampler.SubsetRandomSampler(testing_idx)\n",
        "#dataloaders for training and testing\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = training_sampler)\n",
        "testing_dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, sampler = testing_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdL61bkjOdoM",
        "colab_type": "text"
      },
      "source": [
        "## Training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGdq4B34I0xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKd4KwYOyEfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#directory to save the models\n",
        "models_dir = r'/gdrive/My Drive/models'\n",
        "#file to record metrics  \n",
        "metrics_file_name = 'training_loss.csv' \n",
        "metrics_file_dir = os.path.join(models_dir, metrics_file_name)\n",
        "\n",
        "#loading a previously trained model\n",
        "resume_checkpoint = False\n",
        "checkpoint_file_dir = pass\n",
        "if resume_checkpoint:\n",
        "  checkpoint = torch.load(checkpoint_file_dir)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "else:\n",
        "  start_epoch = 0\n",
        "\n",
        "\n",
        "#training and testing simultaneously\n",
        "total_step = len(dataloader)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    #training epoch\n",
        "    training_epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    for i, (lo_res, hi_res) in enumerate(dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "          continue  \n",
        "        num_batches += 1 \n",
        "        #forward pass \n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        training_epoch_loss += loss.item()\n",
        "        #backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    training_epoch_loss /= num_batches\n",
        "    #save model after training epoch   \n",
        "    checkpoint_file_dir = os.path.join(models_dir, 'EDCNN_checkpoint_epoch_' + str(epoch + 1) + '.pth.tar' )   \n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, checkpoint_file_dir)\n",
        "    print('Training epoch [{}/{}]'.format(epoch+1, num_epochs)) \n",
        "    \n",
        "    #testing epoch\n",
        "    model.eval()\n",
        "    testing_epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "      for batch, (lo_res, hi_res) in enumerate(testing_dataloader):\n",
        "        #add an extra dimension:\n",
        "        lo_res = utils.var_or_cuda( lo_res.unsqueeze(1) )\n",
        "        hi_res = utils.var_or_cuda(hi_res)\n",
        "        if lo_res.size()[0] != batch_size:\n",
        "            continue\n",
        "        num_batches += 1\n",
        "        outputs = model(lo_res)\n",
        "        loss = criterion(outputs, hi_res.unsqueeze(1))\n",
        "        testing_epoch_loss += loss.item()\n",
        "\n",
        "    testing_epoch_loss /= num_batches\n",
        "    print('Testing epoch [{}/{}]'.format(epoch+1, num_epochs) )     \n",
        "\n",
        "    csv_line = str(training_epoch_loss) + ',' + str(testing_epoch_loss) + '\\n'\n",
        "    with open(metrics_file_dir , 'a+') as file:\n",
        "        file.write(csv_line)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}