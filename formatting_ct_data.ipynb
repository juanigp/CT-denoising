{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notebook to organize the data from the CTs\n",
    "\n",
    "## Note: \n",
    "- this would process every volume in the dataset (which could take a long time)\n",
    "- with all the file and dirs handling there should be checks if the folders or files are already created or bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import ReadWriteXML as rw\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory with all the volumes\n",
    "#volumes_dir = r'C:\\Users\\Juanig\\Desktop\\Desktop_\\ct images'\n",
    "volumes_dir = r'C:\\Users\\Juan Pisula\\Desktop\\ct_images\\patches'\n",
    "#directory with the ground truth files\n",
    "gt_dir = os.path.join(volumes_dir, 'Ground Truth')\n",
    "#directory with the lo res files\n",
    "lr_dir = os.path.join(volumes_dir, 'Low resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets extract smaller patches out of every volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract patches of v*v*v with a stride of s and return a list of numpy arrays with them\n",
    "def extract_patches(nparray, volume_size, stride):\n",
    "    v_x, v_y, v_z = volume_size[0], volume_size[1], volume_size[2]\n",
    "    s_x, s_y, s_z = stride[0], stride[1], stride[2]\n",
    "\n",
    "    patches_list = []\n",
    "    for i in range(v_x - 1, nparray.shape[0] + s_x, s_x):\n",
    "        for j in range (v_y - 1, nparray.shape[1] + s_y, s_y):\n",
    "            for k in range(v_z - 1, nparray.shape[2] + s_z, s_z):\n",
    "\n",
    "                i_max = min(i, nparray.shape[0] - 1)\n",
    "                j_max = min(j, nparray.shape[1] - 1)\n",
    "                k_max = min(k, nparray.shape[2] - 1)\n",
    "                i_min, j_min, k_min = i_max -(v_x - 1), j_max -(v_y - 1), k_max -(v_z - 1)\n",
    "\n",
    "                patch = np.zeros([v_x, v_y, v_z])\n",
    "                patch[0 : v_x, 0 : v_y, 0 : v_z] = nparray[i_min:(i_max+1), j_min:(j_max+1), k_min:(k_max+1)]\n",
    "                patches_list.append(patch)     \n",
    "    \n",
    "    return patches_list    \n",
    "\n",
    "\"\"\"\n",
    "open a .xml volume and extract the patches\n",
    "the chosen volume size is 23 x 47 x 47 given that the voxel size has a volume of 0.34 * 0.172 * 0.172 [mm]\n",
    "\"\"\"\n",
    "\n",
    "def open_volume_and_extract (file_path):\n",
    "    itk_volume = rw.OpenXML(file_path, kind = 'Slices')\n",
    "    np_volume = sitk.GetArrayFromImage(itk_volume)\n",
    "    volume_size = (23, 47, 47)\n",
    "    stride = (10, 20, 20)\n",
    "    patches = extract_patches(np_volume, volume_size, stride)\n",
    "    return patches\n",
    "\n",
    "#True if the patch does not contain any NaN value\n",
    "def is_valid_patch(patch):\n",
    "    result = not (np.any(np.isnan(patch)))\n",
    "    return result\n",
    "\n",
    "#Save a list of patches if they are valid\n",
    "def save_patches (patches, new_dir):       \n",
    "    i = 0\n",
    "    for patch in patches:\n",
    "        if is_valid_patch(patch):\n",
    "            #remember, patch is a numpy array!\n",
    "            #and their filename is an integer:\n",
    "            #the corresponding hi res patch of a lo res pair would have the same file name! (and stored in a different directory)\n",
    "            torch.save(patch, os.path.join(new_dir, str(i) + '.pt'))\n",
    "        i += 1    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Some low res volume patches have NaN values on it corresponding to a border, this wouldnt make representative data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rw.OpenXML(os.path.join(lr_dir, '5a_100_FBPPhil_Scan1Slices.xml'), kind = 'Slices' )\n",
    "test_np = sitk.GetArrayFromImage(test)\n",
    "test_patches = extract_patches(test_np, (23, 47, 47), (10, 20, 20) ) \n",
    "plt.imshow(test_patches[0][0][:][:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the _ground truth_ volumes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the files to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#every file in the directory gt_dir\n",
    "gt_files = [f for f in os.listdir(gt_dir) if os.path.isfile(os.path.join(gt_dir, f))]\n",
    "gt_files = list(filter(lambda x: 'Slices.xml' in x, gt_files))\n",
    "gt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_patches_dir = os.path.join(volumes_dir, 'Ground Truth patches')\n",
    "\n",
    "if not os.path.exists(gt_patches_dir):\n",
    "    os.makedirs(gt_patches_dir)\n",
    "\n",
    "for f in gt_files:                    \n",
    "    new_dir = os.path.join(gt_patches_dir, f.replace('.xml', ''))\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    patches = open_volume_and_extract(os.path.join(gt_dir, f))\n",
    "    save_patches(patches, new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing all the _low resolution_ volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_res_config = '100_FBPPhil'\n",
    "hi_res_config = '500FBP' #'XCT'\n",
    "\n",
    "lr_files = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir,f))]\n",
    "lr_files = list(filter(lambda x: lo_res_config in x, lr_files)) \n",
    "lr_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_patches_dir = os.path.join(volumes_dir, 'Low resolution patches')\n",
    "\n",
    "if not os.path.exists(lr_patches_dir) :\n",
    "    os.makedirs(lr_patches_dir)\n",
    "\n",
    "for f in lr_files:\n",
    "    new_dir = os.path.join(lr_patches_dir, f.replace('.xml', ''))\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)       \n",
    "    patches = open_volume_and_extract(os.path.join(lr_dir, f))\n",
    "    save_patches(patches, new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a .csv with the directories of the low res patches and their corresponding hi res patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a pandas dataframe with all the files in the input directories\n",
    "def patches_to_dataframe(lo_res_dir, high_res_dir):\n",
    "    #list with every file (patch) in the lo_res_dir\n",
    "    lo_res_files = os.listdir(lo_res_dir)\n",
    "    pairs = []\n",
    "    #for every patch in the folder lo_res_dir\n",
    "    for file in lo_res_files:\n",
    "        #directory of the lo res patch\n",
    "        lo_res_file_dir = os.path.join(lo_res_dir, file)\n",
    "        #directory where the ground truth patch should be\n",
    "        hi_res_file_dir = os.path.join(high_res_dir, file)\n",
    "        #if the file exists, append to the list\n",
    "        if os.path.exists ( hi_res_file_dir ):\n",
    "            pairs.append( (lo_res_file_dir, hi_res_file_dir) )\n",
    "    #make the dataframe\n",
    "    df_out = pd.concat( [ pd.DataFrame( [ [ pair[0], pair[1] ] ], columns=['Lo Res Patch', 'Hi Res Patch']) for pair in pairs] ,ignore_index=True)            \n",
    "    return df_out\n",
    "\n",
    "#recursive function to get a list of dataframes \n",
    "#the input arg is a list of pairs (tuples) of the lo res patches directory and their corresponding hi res patches directory =\n",
    "def _files_to_csv(dirs_list):\n",
    "    if len(dirs_list) == 1:\n",
    "        pair = dirs_list.pop(0)\n",
    "        return [ patches_to_dataframe( pair[0], pair[1]  ) ]\n",
    "    else:\n",
    "        pair = dirs_list.pop(0)\n",
    "        recursive_case = _files_to_csv(dirs_list)\n",
    "        recursive_case.append( patches_to_dataframe( pair[0], pair[1]  ) )\n",
    "        return recursive_case\n",
    "    \n",
    "#make a .csv with the directories of the pairs of patches \n",
    "#lo_res_config can be, for example: '100_FBPPhil'\n",
    "#hi_res config can be, for example: '500FBP'\n",
    "def make_csv_for_dataset(volumes_dir, lo_res_config, hi_res_config ):\n",
    "    lr_dir = os.path.join(volumes_dir, 'Low resolution patches')\n",
    "    gt_dir = os.path.join(volumes_dir, 'Ground Truth patches')   \n",
    "    \n",
    "    #get the directories of the desired patches \n",
    "    lr_dirs = [f for f in os.listdir(lr_dir) if os.path.isdir(os.path.join(lr_dir,f))]\n",
    "    lr_dirs = list(filter(lambda x: lo_res_config in x, lr_dirs)) \n",
    "    \n",
    "    gt_dirs = [f for f in os.listdir(gt_dir) if os.path.isdir(os.path.join(gt_dir,f))]\n",
    "    gt_dirs = list(filter(lambda x: hi_res_config in x, gt_dirs))\n",
    "    \n",
    "    pairs = []\n",
    "\n",
    "    for lr_folder in lr_dirs:\n",
    "        #string with the \"patient\"\n",
    "        patient = lr_folder.split('_',1)[0]\n",
    "        #get the folder of the corresponding ground truth volume for that patient\n",
    "        gt_folder = list(filter(lambda x: patient in x, gt_dirs))[0]\n",
    "        #make the tuple with the pair of dirs\n",
    "        pair = ( os.path.join(lr_dir,lr_folder), os.path.join(gt_dir, gt_folder)   )\n",
    "        #append to list\n",
    "        pairs.append(pair)\n",
    "     \n",
    "    #get the list of dataframes\n",
    "    dataframes = _files_to_csv(pairs)  \n",
    "    #concatenate dataframes\n",
    "    csv = pd.concat(dataframes , ignore_index = True)\n",
    "    #save as .csv\n",
    "    csv_name = lo_res_config + '_' + hi_res_config + '.csv'\n",
    "    csv_dir = os.path.join(volumes_dir, csv_name)\n",
    "    csv.to_csv(csv_dir, index = False)\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = make_csv_for_dataset(volumes_dir, lo_res_config, hi_res_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11000\n",
    "print(csv.iloc[i]['Lo Res Patch'])\n",
    "print(csv.iloc[i]['Hi Res Patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
